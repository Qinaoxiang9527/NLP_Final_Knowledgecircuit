{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本体知识电路组件分析\n",
    "\n",
    "本笔记本用于分析本体知识电路的组件，包括模型中处理本体知识的关键部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from functools import partial\n",
    "from typing import List, Optional, Union\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import torch\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "from jaxtyping import Float\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "import seaborn as sns\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer, SVDInterpreter\n",
    "from acdc.knowledge.ontology_dataset import OntologyDataset\n",
    "from acdc.knowledge.utils import get_all_knowledge_things\n",
    "\n",
    "# 禁用自动微分\n",
    "torch.set_grad_enabled(False)\n",
    "print(\"已禁用自动微分\")\n",
    "plt.rcParams['font.sans-serif']=['Times New Roman']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 辅助函数定义\n",
    "\n",
    "首先定义一些辅助函数，用于处理令牌和查找文本范围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokenizer, token_array):\n",
    "    \"\"\"解码令牌数组为文本\"\"\"\n",
    "    if hasattr(token_array, \"shape\") and len(token_array.shape) > 1:\n",
    "        return [decode_tokens(tokenizer, row) for row in token_array]\n",
    "    return [tokenizer.decode([t]) for t in token_array]\n",
    "\n",
    "def find_sublist_index(main_list, sub_list):\n",
    "    \"\"\"在主列表中查找子列表的索引\"\"\"\n",
    "    main_list = main_list.tolist()\n",
    "    sub_list = sub_list.tolist()\n",
    "    index = main_list.index(sub_list[0])\n",
    "    return (index, index+len(sub_list))\n",
    "\n",
    "def check_subrange(tokens, sublist, range):\n",
    "    \"\"\"检查指定范围内的令牌是否与子列表匹配\"\"\"\n",
    "    subtokens = tokens[range[0]:range[1]]\n",
    "    assert len(sublist) == len(subtokens)\n",
    "    for x, y in zip(sublist, subtokens):\n",
    "        if x == y:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_token_range(tokenizer, token_array, substring):\n",
    "    \"\"\"查找子字符串在令牌数组中的范围\"\"\"\n",
    "    toks = decode_tokens(tokenizer, token_array)\n",
    "    whole_string = \"\".join(toks)\n",
    "    char_loc = whole_string.index(substring)\n",
    "    loc = 0\n",
    "    tok_start, tok_end = None, None\n",
    "    for i, t in enumerate(toks):\n",
    "        loc += len(t)\n",
    "        if tok_start is None and loc > char_loc:\n",
    "            tok_start = i\n",
    "        if tok_end is None and loc >= char_loc + len(substring):\n",
    "            tok_end = i + 1\n",
    "            break\n",
    "    return (tok_start, tok_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 本体组件分析器\n",
    "\n",
    "创建一个专门用于分析本体知识组件的类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyComponentAnalyzer:\n",
    "    def __init__(self, model, prompt, answer, subject) -> None:\n",
    "        \"\"\"初始化本体组件分析器\n",
    "        \n",
    "        Args:\n",
    "            model: 模型\n",
    "            prompt: 提示文本\n",
    "            answer: 答案文本\n",
    "            subject: 主题文本\n",
    "        \"\"\"\n",
    "        # 提示文本列表\n",
    "        self.prompts = [prompt]\n",
    "        tokens = model.to_tokens(self.prompts, prepend_bos=True)\n",
    "        str_tokens = model.to_str_tokens(self.prompts, prepend_bos=True)\n",
    "        self.tokens = tokens\n",
    "        self.str_tokens = str_tokens\n",
    "        \n",
    "        # 查找主题在令牌中的位置\n",
    "        if 'llama' in model.cfg.model_name:\n",
    "            subject_range = find_sublist_index(tokens[0], model.to_tokens(subject, prepend_bos=False)[0])\n",
    "            assert check_subrange(tokens[0], model.to_tokens(subject, prepend_bos=False)[0], subject_range)\n",
    "        else:\n",
    "            subject_range = find_token_range(model.tokenizer, tokens[0], subject)\n",
    "            \n",
    "        # 答案令牌\n",
    "        self.answer_token = model.to_tokens(answer, prepend_bos=False)[0][0]\n",
    "        self.subject_range = subject_range\n",
    "        self.subject_last_token = tokens[0][subject_range[-1]-1]\n",
    "        \n",
    "        # 运行模型并获取缓存\n",
    "        self.logits, self.cache = model.run_with_cache(tokens)\n",
    "        self.accum_resid, self.labels = self.cache.accumulated_resid(incl_mid=False, mlp_input=True, return_labels=True, apply_ln=True)\n",
    "        self.heads_out = self.get_heads_out(model)\n",
    "        self.mlp_out = self.get_mlp_out(model)\n",
    "    \n",
    "    def get_min_rank_at_subject(self, W_U, token_num):\n",
    "        \"\"\"获取主题位置处令牌的最小排名\"\"\"\n",
    "        last_token_accum = self.accum_resid[:, 0, self.subject_range[0]:self.subject_range[1], :] \n",
    "        layers_unembedded = einsum(\n",
    "                \"layer ... d_model, d_model d_vocab -> layer ... d_vocab\",\n",
    "                last_token_accum,\n",
    "                W_U,\n",
    "            )\n",
    "        sorted_indices = torch.argsort(layers_unembedded, dim=2, descending=True)\n",
    "        rank_answer = (sorted_indices == token_num).nonzero(as_tuple=True)[2].view(layers_unembedded.size(0),-1)\n",
    "        return rank_answer.min(dim=-1)[0]\n",
    "    \n",
    "    def get_token_rank(self, W_U, token_num, pos=-1):\n",
    "        \"\"\"获取指定位置处令牌的排名\"\"\"\n",
    "        last_token_accum = self.accum_resid[:, 0, pos, :] \n",
    "        layers_unembedded = einsum(\n",
    "                \"layer d_model, d_model d_vocab -> layer d_vocab\",\n",
    "                last_token_accum,\n",
    "                W_U,\n",
    "            )\n",
    "        sorted_indices = torch.argsort(layers_unembedded, dim=1, descending=True)\n",
    "        rank_answer = (sorted_indices == token_num).nonzero(as_tuple=True)[1]\n",
    "        return rank_answer\n",
    "    \n",
    "    def get_token_logits(self, model, tokens, pos=-1):\n",
    "        \"\"\"获取指定位置处令牌的logits\"\"\"\n",
    "        answer_residual_directions = model.tokens_to_residual_directions(tokens)\n",
    "        logit_diff_directions = (\n",
    "            answer_residual_directions[:, ]\n",
    "        )\n",
    "        if len(logit_diff_directions.shape) == 1:\n",
    "            logit_diff_directions = logit_diff_directions.unsqueeze(0)\n",
    "        scaled_residual_stack = self.cache.apply_ln_to_stack(\n",
    "                self.accum_resid, layer=-1, pos_slice=-1\n",
    "            )\n",
    "        return einsum(\n",
    "                \"... batch d_model, batch d_model -> ...\",\n",
    "                scaled_residual_stack,\n",
    "                logit_diff_directions,\n",
    "            )\n",
    "    \n",
    "    def get_token_probability(self, model, tokens, pos=-1):\n",
    "        \"\"\"获取指定位置处令牌的概率\"\"\"\n",
    "        last_token_accum = self.accum_resid[:, 0, pos, :] \n",
    "        layers_unembedded = einsum(\n",
    "                \"layer d_model, d_model d_vocab -> layer d_vocab\",\n",
    "                last_token_accum,\n",
    "                model.W_U,\n",
    "            )\n",
    "        probs = layers_unembedded.softmax(dim=-1)\n",
    "        return probs[:, tokens]\n",
    "    \n",
    "    def get_heads_out(self, model, pos_slice=-1):\n",
    "        \"\"\"获取所有注意力头的输出\"\"\"\n",
    "        per_head_residual, labels = self.cache.stack_head_results(\n",
    "            layer=-1, pos_slice=pos_slice, return_labels=True, apply_ln=True\n",
    "        )\n",
    "        heads_out = {}\n",
    "        for index, label in enumerate(labels):\n",
    "            # 设置标签\n",
    "            layer = index // model.cfg.n_heads\n",
    "            head_index = index % model.cfg.n_heads\n",
    "            assert f\"L{layer}H{head_index}\" == label\n",
    "            heads_out[label] = per_head_residual[index, :]\n",
    "        return heads_out\n",
    "    \n",
    "    def get_mlp_out(self, model, pos_slice=-1):\n",
    "        \"\"\"获取所有MLP层的输出\"\"\"\n",
    "        per_layer_residual, labels = self.cache.decompose_resid(\n",
    "            mode='mlp', layer=-1, pos_slice=-1, return_labels=True, apply_ln=True\n",
    "        )\n",
    "        mlp_out = {}\n",
    "        for x, y in zip(per_layer_residual, labels):\n",
    "            mlp_out[y] = x\n",
    "        return mlp_out\n",
    "    \n",
    "    def get_component_logits(self, output, model):\n",
    "        \"\"\"获取组件的logits并打印前10个令牌\"\"\"\n",
    "        layers_unembedded = einsum(\n",
    "                \" ... d_model, d_model d_vocab -> ... d_vocab\",\n",
    "                output,\n",
    "                model.W_U,\n",
    "            )\n",
    "        sorted_indices = torch.argsort(layers_unembedded, dim=1, descending=True)\n",
    "        temp_logits = layers_unembedded[0]\n",
    "        tmp_sorted_indices = sorted_indices[0]\n",
    "        for i in range(10):\n",
    "            print(\n",
    "                f\"第 {i} 个令牌. Logit: {temp_logits[tmp_sorted_indices[i]].item():5.2f} 令牌: |{model.to_string(tmp_sorted_indices[i])}|\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 可视化函数\n",
    "\n",
    "定义用于可视化组件输出的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_output_pattern_with_text(component, model, top_k=10):\n",
    "    \"\"\"绘制组件输出模式并显示文本\"\"\"\n",
    "    layers_unembedded = einsum(\n",
    "            \" ... d_model, d_model d_vocab -> ... d_vocab\",\n",
    "            component,\n",
    "            model.W_U,\n",
    "        )\n",
    "    sorted_indices = torch.argsort(layers_unembedded, dim=1, descending=True)\n",
    "    temp_logits = layers_unembedded[0]\n",
    "    tmp_sorted_indices = sorted_indices[0]\n",
    "    \n",
    "    # 获取前top_k个令牌及其logits\n",
    "    tokens = [model.to_string(tmp_sorted_indices[i]) for i in range(top_k)]\n",
    "    logits = [temp_logits[tmp_sorted_indices[i]].item() for i in range(top_k)]\n",
    "    \n",
    "    # 创建DataFrame并绘制条形图\n",
    "    df = pd.DataFrame({\n",
    "        '令牌': tokens,\n",
    "        'Logit': logits\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Logit', y='令牌', data=df)\n",
    "    plt.title('组件输出模式')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_component_attribution(analyzer, model, component_name):\n",
    "    \"\"\"绘制组件归因图\"\"\"\n",
    "    if component_name.startswith('L') and 'H' in component_name:\n",
    "        # 注意力头组件\n",
    "        component_output = analyzer.heads_out[component_name]\n",
    "    elif component_name.startswith('mlp'):\n",
    "        # MLP组件\n",
    "        component_output = analyzer.mlp_out[component_name]\n",
    "    else:\n",
    "        raise ValueError(f\"未知的组件名称: {component_name}\")\n",
    "    \n",
    "    # 获取组件的logits\n",
    "    print(f\"组件 {component_name} 的输出模式:\")\n",
    "    analyzer.get_component_logits(component_output, model)\n",
    "    \n",
    "    # 绘制输出模式\n",
    "    df = draw_output_pattern_with_text(component_output, model)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载模型和数据\n",
    "\n",
    "加载预训练模型和本体知识数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"gpt2-medium\"\n",
    "model_path = \"/mnt/workspace/qinaoxiang/KnowledgeCircuits-main/acdc/models/gpt2-medium\"\n",
    "\n",
    "print(f\"加载模型: {model_name}\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False\n",
    ")\n",
    "model.cfg.use_split_qkv_input = True\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True\n",
    "\n",
    "print(\"模型加载完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 加载本体知识数据集\n",
    "\n",
    "加载本体知识数据集并准备示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载本体知识数据集\n",
    "data_path = \"/mnt/workspace/qinaoxiang/KnowledgeCircuits-main/data\"\n",
    "relation_name = \"ontology_class_hierarchy\"\n",
    "num_examples = 5\n",
    "\n",
    "print(f\"加载本体知识数据集: {relation_name}\")\n",
    "all_knowledge_things = get_all_knowledge_things(\n",
    "    num_examples=num_examples,\n",
    "    device=device,\n",
    "    model=model_name,\n",
    "    model_path=model_path,\n",
    "    knowledge_type=\"ontology\",\n",
    "    data_path=data_path,\n",
    "    relation_name=relation_name,\n",
    "    reverse=False,\n",
    "    data_seed=42,\n",
    "    metric_name=\"match_nll\",\n",
    ")\n",
    "\n",
    "# 提取数据\n",
    "validation_data = all_knowledge_things.validation_data\n",
    "validation_labels = all_knowledge_things.validation_labels\n",
    "\n",
    "print(f\"加载了 {validation_data.shape[0]} 个示例\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 分析示例\n",
    "\n",
    "选择一个示例进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择第一个示例\n",
    "example_idx = 0\n",
    "example_tokens = validation_data[example_idx:example_idx+1]\n",
    "\n",
    "# 解码示例\n",
    "example_text = model.to_string(example_tokens[0])\n",
    "print(f\"示例文本:\\n{example_text}\")\n",
    "\n",
    "# 运行模型\n",
    "logits, cache = model.run_with_cache(example_tokens)\n",
    "probs = torch.softmax(logits[:, -1], dim=-1)\n",
    "\n",
    "# 获取前5个预测\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probs[0], top_k)\n",
    "\n",
    "print(f\"\\n模型的前 {top_k} 个预测:\")\n",
    "for i in range(top_k):\n",
    "    token = model.to_string(top_indices[i])\n",
    "    prob = top_probs[i].item()\n",
    "    print(f\"{i+1}. 令牌: '{token}', 概率: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 创建组件分析器\n",
    "\n",
    "为选定的示例创建组件分析器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从示例中提取主题和答案\n",
    "# 注意：这里需要根据实际数据格式调整提取方法\n",
    "example_parts = example_text.split('\\n')\n",
    "prompt_text = example_text\n",
    "\n",
    "# 假设主题是类名，答案是其父类\n",
    "# 这里需要根据实际数据调整\n",
    "subject = \"类名\"  # 示例，需要替换为实际主题\n",
    "answer = top_indices[0].item()  # 使用模型预测的最可能答案\n",
    "\n",
    "# 创建组件分析器\n",
    "analyzer = OntologyComponentAnalyzer(\n",
    "    model=model,\n",
    "    prompt=prompt_text,\n",
    "    answer=model.to_string(answer),\n",
    "    subject=subject\n",
    ")\n",
    "\n",
    "print(\"组件分析器创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 分析关键组件\n",
    "\n",
    "分析对本体知识处理最重要的组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析注意力头的贡献\n",
    "print(\"分析注意力头的贡献...\")\n",
    "\n",
    "# 计算每个注意力头对答案的贡献\n",
    "head_contributions = {}\n",
    "for head_name, head_output in analyzer.heads_out.items():\n",
    "    # 计算头输出与答案方向的点积\n",
    "    answer_dir = model.tokens_to_residual_directions([answer])[0]\n",
    "    contribution = torch.sum(head_output * answer_dir).item()\n",
    "    head_contributions[head_name] = contribution\n",
    "\n",
    "# 按贡献排序\n",
    "sorted_heads = sorted(head_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 显示贡献最大的10个头\n",
    "print(\"贡献最大的10个注意力头:\")\n",
    "for i, (head_name, contribution) in enumerate(sorted_heads[:10]):\n",
    "    print(f\"{i+1}. {head_name}: {contribution:.4f}\")\n",
    "\n",
    "# 分析MLP层的贡献\n",
    "print(\"\\n分析MLP层的贡献...\")\n",
    "\n",
    "# 计算每个MLP层对答案的贡献\n",
    "mlp_contributions = {}\n",
    "for mlp_name, mlp_output in analyzer.mlp_out.items():\n",
    "    # 计算MLP输出与答案方向的点积\n",
    "    answer_dir = model.tokens_to_residual_directions([answer])[0]\n",
    "    contribution = torch.sum(mlp_output * answer_dir).item()\n",
    "    mlp_contributions[mlp_name] = contribution\n",
    "\n",
    "# 按贡献排序\n",
    "sorted_mlps = sorted(mlp_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 显示贡献最大的10个MLP层\n",
    "print(\"贡献最大的10个MLP层:\")\n",
    "for i, (mlp_name, contribution) in enumerate(sorted_mlps[:10]):\n",
    "    print(f\"{i+1}. {mlp_name}: {contribution:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 可视化关键组件\n",
    "\n",
    "可视化对本体知识处理最重要的组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化贡献最大的注意力头\n",
    "top_head_name = sorted_heads[0][0]\n",
    "print(f\"可视化贡献最大的注意力头: {top_head_name}\")\n",
    "head_df = plot_component_attribution(analyzer, model, top_head_name)\n",
    "\n",
    "# 可视化贡献最大的MLP层\n",
    "top_mlp_name = sorted_mlps[0][0]\n",
    "print(f\"\\n可视化贡献最大的MLP层: {top_mlp_name}\")\n",
    "mlp_df = plot_component_attribution(analyzer, model, top_mlp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 分析组件交互\n",
    "\n",
    "分析关键组件之间的交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取前3个最重要的注意力头和MLP层\n",
    "top_heads = [name for name, _ in sorted_heads[:3]]\n",
    "top_mlps = [name for name, _ in sorted_mlps[:3]]\n",
    "\n",
    "print(f\"分析前3个最重要的组件之间的交互...\")\n",
    "print(f\"注意力头: {', '.join(top_heads)}\")\n