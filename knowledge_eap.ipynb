{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from eap.metrics import logit_diff, direct_logit\n",
    "import transformer_lens.utils as utils\n",
    "from eap.graph import Graph\n",
    "from eap.dataset import EAPDataset\n",
    "from eap.attribute import attribute\n",
    "import time\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline,get_circuit_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "from transformers import LlamaForCausalLM\n",
    "model = HookedTransformer.from_pretrained(LLAMA_2_7B_CHAT_PATH, device=\"cuda\", fold_ln=False, center_writing_weights=False, center_unembed=False)\n",
    "model.cfg.use_split_qkv_input = True\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subject = 'Eiffel Tower'\n",
    "corrupted_subject = 'the Great Walls'\n",
    "clean = f'The official currency of the country where {clean_subject} is loacted in is the'\n",
    "corrupted = f'The official currency of the country where {corrupted_subject} is loacted in is the'\n",
    "assert len(model.to_str_tokens(clean.format(clean_subject))) == len(model.to_str_tokens(corrupted.format(corrupted_subject)))\n",
    "labels = ['Euro','Chinese']\n",
    "country_idx = model.tokenizer(labels[0],add_special_tokens=False).input_ids[0]\n",
    "corrupted_country_idx = model.tokenizer(labels[1],add_special_tokens=False).input_ids[0]\n",
    "# dataset = {k:[] for k in ['clean','country_idx', 'corrupted',  'corrupted_country_idx']}\n",
    "# for k, v in zip(['clean', 'country_idx', 'corrupted', 'corrupted_country_idx'], [clean, country_idx, corrupted, corrupted_country_idx]):\n",
    "#     dataset[k].append(v)\n",
    "# df2 = pd.DataFrame.from_dict(dataset)\n",
    "# df2.to_csv(f'capital_city.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [[country_idx, corrupted_country_idx]]\n",
    "label = torch.tensor(label)\n",
    "data = ([clean],[corrupted],label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = EAPDataset(filename='capital_city.csv',task='fact-retrieval')\n",
    "# dataloader = ds.to_dataloader(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1592881/1592881 [00:01<00:00, 1062625.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序执行时间：43.55915355682373秒\n"
     ]
    }
   ],
   "source": [
    "g = Graph.from_model(model)\n",
    "start_time = time.time()\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g, data, partial(logit_diff, loss=True, mean=True), method='EAP-IG-case', ig_steps=100)\n",
    "# attribute(model, g, data, partial(direct_logit, loss=True, mean=True), method='EAP-IG-case', ig_steps=30)\n",
    "# attribute(model, g, dataloader, partial(logit_diff, loss=True, mean=True), method='EAP-IG', ig_steps=30)\n",
    "g.apply_topn(5000, absolute=True)\n",
    "g.prune_dead_nodes()\n",
    "\n",
    "g.to_json('graph.json')\n",
    "\n",
    "gz = g.to_graphviz()\n",
    "gz.draw(f'graph.png', prog='dot')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"程序执行时间：{execution_time}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_logits(logits, model, answer_token, top_k=10):\n",
    "    logits = utils.remove_batch_dim(logits)\n",
    "    # print(heads_out[head_name].shape)\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    token_probs = probs[-1]\n",
    "    answer_str_token = model.to_string(answer_token)\n",
    "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
    "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
    "    correct_rank = torch.arange(len(sorted_token_values))[\n",
    "        (sorted_token_values == answer_token).cpu()\n",
    "    ].item()\n",
    "    # answer_ranks = []\n",
    "    # answer_ranks.append((answer_str_token, correct_rank))\n",
    "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
    "    # rprint gives rich text printing\n",
    "    rprint(\n",
    "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
    "    )\n",
    "    for i in range(top_k):\n",
    "        print(\n",
    "            f\"Top {i}th token. Logit: {logits[-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
    "        )\n",
    "    # rprint(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.94</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56.56</span><span style=\"font-weight: bold\">% Token: |Euro|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.94\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m56.56\u001b[0m\u001b[1m% Token: |Euro|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.94 Prob: 56.56% Token: |Euro|\n",
      "Top 1th token. Logit: 15.96 Prob: 21.39% Token: |French|\n",
      "Top 2th token. Logit: 14.06 Prob:  3.18% Token: |_|\n",
      "Top 3th token. Logit: 13.95 Prob:  2.85% Token: |euro|\n",
      "Top 4th token. Logit: 13.91 Prob:  2.74% Token: |Eu|\n"
     ]
    }
   ],
   "source": [
    "logits = get_circuit_logits(model, g, data)\n",
    "get_component_logits(logits, model, answer_token=model.to_tokens('Euro',prepend_bos=False)[0], top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original performance was 10.043922424316406; the circuit's performance is 6.337347984313965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = evaluate_baseline(model, dataloader, partial(logit_diff, loss=False, mean=False)).mean().item()\n",
    "results = evaluate_graph(model, g, dataloader, partial(logit_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
